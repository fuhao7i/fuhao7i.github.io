I"š<blockquote>
  <ul>
    <li>Due to the improper selection of the learning rate, the weight update is larger.</li>
    <li>There is a lot of noise in the prepared data, resulting in large differences in target variables.</li>
    <li>The improper selection of the loss function results in the calculation of a larger error value.</li>
  </ul>
</blockquote>

<p>æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±çš„ä¸€ç§å¸¸è§è§£å†³æ–¹æ³•æ˜¯é‡æ–°ç¼©æ”¾è¯¯å·®å¯¼æ•°ï¼Œé€šè¿‡ç½‘ç»œåå‘ä¼ æ’­è¯¯å·®å¯¼æ•°ï¼Œç„¶åä½¿ç”¨å®ƒæ¥æ›´æ–°æƒé‡ã€‚é€šè¿‡é‡æ–°ç¼©æ”¾è¯¯å·®å¯¼æ•°ï¼Œæƒé‡çš„æ›´æ–°ä¹Ÿå°†è¢«é‡æ–°ç¼©æ”¾ï¼Œä»è€Œå¤§å¤§é™ä½äº†ä¸Šæº¢æˆ–ä¸‹æº¢(NaN: not a number, Inf: infinity)çš„å¯èƒ½æ€§ã€‚æ›´æ–°è¯¯å·®å¯¼æ•°çš„ä¸»è¦æ–¹æ³•æœ‰ä¸¤ç§ï¼š</p>

<ul>
  <li>æ¢¯åº¦ç¼©æ”¾ Gradient Scaling</li>
  <li>æ¢¯åº¦è£å‰ª Gradient Clipping</li>
</ul>

<p>æ¢¯åº¦ç¼©æ”¾æ¶‰åŠå¯¹è¯¯å·®æ¢¯åº¦å‘é‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥ä½¿å‘é‡èŒƒæ•°å¤§å°ç­‰äºå®šä¹‰çš„å€¼ï¼Œä¾‹å¦‚1.0ã€‚åªè¦å®ƒä»¬è¶…è¿‡é˜ˆå€¼ï¼Œå°±é‡æ–°ç¼©æ”¾å®ƒä»¬ã€‚å¦‚æœæ¸å˜è¶…å‡ºäº†é¢„æœŸèŒƒå›´ï¼Œåˆ™æ¸å˜è£å‰ªä¼šå¼ºåˆ¶å°†æ¸å˜å€¼ï¼ˆé€ä¸ªå…ƒç´ ï¼‰å¼ºåˆ¶ä¸ºç‰¹å®šçš„æœ€å°å€¼æˆ–æœ€å¤§å€¼ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ç®€ç§°ä¸ºæ¢¯åº¦è£å‰ªã€‚</p>

<p><code class="language-plaintext highlighter-rouge">å®ƒæ˜¯ä¸€ç§ä»…è§£å†³è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ•°å€¼ç¨³å®šæ€§ï¼Œè€Œä¸èƒ½æ”¹è¿›ç½‘ç»œæ€§èƒ½çš„æ–¹æ³•ã€‚</code></p>

<h2 id="implementation-by-torch">implementation by torch</h2>

<p><code class="language-plaintext highlighter-rouge">torch.nn.utils.clip_grad_norm(parameters, max_norm=8, norm_type=2)</code></p>

<p>è¿™ä¸ªå‡½æ•°æ˜¯æ ¹æ®å‚æ•°çš„èŒƒæ•°æ¥è¡¡é‡çš„</p>

<ul>
  <li>parameters: ä¸€ä¸ªåŸºäºå˜é‡çš„è¿­ä»£å™¨ï¼Œä¼šè¿›è¡Œå½’ä¸€åŒ–. an iterable of Variables that will have gradients normalized</li>
  <li>max_norm: æ¢¯åº¦æœ€å¤§èŒƒæ•° max norm of the gradients</li>
  <li>norm_type: è§„å®šèŒƒæ•°çš„ç±»å‹ï¼Œé»˜è®¤ä¸ºL2. type of the used p-norm. Can beâ€™infâ€™for infinity norm</li>
</ul>

<p>returns: å‚æ•°çš„æ€»ä½“èŒƒæ•°(ä½œä¸ºå•ä¸ªå‘é‡æ¥çœ‹)</p>

<h1 id="reference">reference</h1>

<ol>
  <li><a href="https://blog.csdn.net/weixin_39653948/article/details/105962326">ã€è°ƒå‚19ã€‘å¦‚ä½•ä½¿ç”¨æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰é¿å…æ¢¯åº¦çˆ†ç‚¸</a></li>
</ol>
:ET