---
layout:     post
title:      "Neural Network🦖2——优化器详解"
subtitle:   " \"优化器的作用与实现原理\""
date:       2021-02-20 14:17:00
author:     "fuhao7i"
header-img: "img/in-post/nn.jpg"
catalog: true
tags:
    - Neural Network🦖
---

# 2. lr_scheduler 学习率调整策略

### 2.1 ReduceLROnPlateau

目前不依赖epoch更新lr的只有`torch.optim.lr_scheduler.ReduceLROnPlateau`.

### 2.2 StepLR

**grammar**

```python
class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)
```

**update strategy**

每过一个`step_size`个epoch，做一次更新:

$$
\large new\_lr = initial\_lr * gamma^{epoch//step\_size} \tag 1
$$

**parameters**

1. optimizer: 要更改的优化器
2. step_size: 每训练step_size个epoch，更新一次权重
3. gamma: 更新lr的乘法因子
4. last_epoch: 最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。默认为-1表示从头开始训练，即从epoch=1开始

`e.g`

```python
import torch
import torch.nn as nn
from torch.optim.lr_scheduler import StepLR
import itertools


initial_lr = 0.1

class model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)

    def forward(self, x):
        pass

net_1 = model()

optimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)
scheduler_1 = StepLR(optimizer_1, step_size=3, gamma=0.1)

print("初始化的学习率：", optimizer_1.defaults['lr'])

for epoch in range(1, 11):
    # train

    optimizer_1.zero_grad()
    optimizer_1.step()
    print("第%d个epoch的学习率：%f" % (epoch, optimizer_1.param_groups[0]['lr']))
    scheduler_1.step()
```

### 1. 传入两个模型参数
`字典形式`
```python
optimizer = torch.optim.SGD([
                {'params': model.parameters()},
                {'params': lossnet.parameters(), 'lr': 1e-4}
            ], lr, momentum=0.9)
```
# 引用:
1. [pytorch优化器传入两个模型的参数/已不同的学习速率训练模型](https://blog.csdn.net/u011622208/article/details/90698688)