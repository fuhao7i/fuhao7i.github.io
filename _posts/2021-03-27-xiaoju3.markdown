---
layout:     post
title:      "小菊的语义分割3🌼——数据预处理及像素级分类实现原理"
subtitle:   " \"数据标签预处理, Softmax进行像素级分类原理\""
date:       2021-03-27 14:00:00
author:     "fuhao7i"
header-img: "img/in-post/xiaoju.jpg"
catalog: true
tags:
    - 小菊的语义分割🌼
---

# 1. 数据预处理

`思路:` 读取train.txt文件，获取训练图像及对应标签的文件路径，读取图像，将图像转化为`tensor`之后，`resize`调整图像尺寸大小并进行`归一化处理`，之后也可通过旋转，色偏，增加噪声等方式进行`数据增强`。注意要保证图像和标签的处理一致。

<img src='https://img-blog.csdnimg.cn/20210327141350996.png' center>

`padding可以使图像在resize时不失真`

# 2. label map 标签映射

<img src='https://img-blog.csdnimg.cn/20210328145544433.png' center>

如图就是我们的语义分割标签图像，相同颜色(像素值)的像素点代表的是同一类物体。假设像我们这个标签图像所展示的那样，我们需要分割出来图片中的猫和狗，那对我们的语义分割任务来说就是总共要分3类：0 背景；1 猫；2 狗；因此，我们需要创建一个`[Height, Width, N_classes]`数组来表示每一个像素点的类别；如下图所示：

<img src='https://img-blog.csdnimg.cn/20210328145242405.png' center>

`label[0, 0] = [1, 0, 0]` 说明[0, 0]位置是背景，`label[1, 1] = [0, 1, 0]`说明[1, 1]这个像素点属于猫。我们`Reshape`之后好像更方便大家理解:

```python
seg_labels = np.reshape(seg_labels, (-1,NCLASSES))
```

```Bash

 [[1, 0, 0],
  [1, 0, 0],
  [1, 0, 0],
  [1, 0, 0],
  [1, 0, 0],
  [0, 1, 0],
  [0, 0, 1],
  [0, 0, 1],
     ...
           ]
```

这就是我们最后用来和预测结果计算损失的数据啦🌼`相信大家对为什么这样做还有点云里雾里的感觉，那么接下来就让我们揭开语义分割的神秘面纱吧🌼`

# 3. 像素级分类原理

了解完lable的具体格式之后，我们来看一下网络的最后几层设计:

```python
x = Conv2D(n_classes,(1,1), padding='vaild' )(x)
x = Reshape((-1,n_classes))(x)
output = Softmax()(x)
```

这个和我们label的处理是一致的，输出的是每一个像素点属于哪一类的概率，如下图：

<img src='https://img-blog.csdnimg.cn/20210328152456675.png' center>

`tensor表示:`
```Bash
       Output              Label
 [[0.4, 0.3, 0.3],      [[1, 0, 0],
  [0.7, 0.2, 0.1],       [1, 0, 0],
  [0.8, 0.2, 0.0],       [1, 0, 0],
  [0.8, 0.1, 0.1],       [1, 0, 0],
        ...                 ...
                 ]
```

将我们处理之后的label和预测得到的结果传给我们的损失函数就能计算出loss了，这样我们就实现了像素级的分类————也就是`语义分割`了🌼

**下面是包含了数据预处理，损失定义等整个模型训练过程的train.py文件，大家稍作修改就可以训练自己的语义分割模型了🌼**

**[Keras实现](https://keras.io/zh/models/model/)**

```python
import numpy as np
from PIL import Image

import keras
from keras import backend as K
from keras.optimizers import Adam
from keras.layers import Lambda
from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

# 标签像素值对应的物体类别, 0为背景
CLASSES = {
    '[0 0 0]' : 0, # 背景
    '[7 7 7]' : 1,
    '[26 26 26]' : 2,
}

NCLASSES = 2
HEIGHT = 576
WIDTH = 576

BATCH_SIZE = 2

# train.txt和val.txt的文件路径
path_train_txt = ''
path_val_txt = ''

# train的图像和标签路径
path_Xtrain = ''
path_Xlabel = ''
# val的图像和标签路径
path_Yval = ''
path_Ylabel = ''


# labels映射
def label_map(labels):
    labelmap = np.zeros((int(HEIGHT),int(WIDTH),NCLASSES))
    for h in range(int(HEIGHT)):
        for w in range(int(WIDTH)):
            if str(labels[h, w]) in CLASSES.keys():
                c = CLASSES[str(labels[h, w])]
            else:
                c = 0
            labelmap[h, w, c] = 1
    return labelmap


def data_generator(mode):
    assert mode in ['train', 'val'], \
        'mode must be ethier \'train\' or \'val\''

    if mode == 'train':
        with open(path_train_txt, 'r') as f:
            lines = f.readlines()
        np.random.shuffle(lines)

        n = len(lines)
        path0 = path_Xtrain
        path1 = path_Xlabel
    else:
        with open(path_val_txt, 'r') as f:
            lines = f.readlines()
        np.random.shuffle(lines)

        n = len(lines)
        path0 = path_Yval
        path1 = path_Ylabel 

    i = 0
    while 1:
        images = []
        labels = []
        for _ in range(BATCH_SIZE):
            if i==0:
                np.random.shuffle(lines)
            name = lines[i].split(';')[0]
            img = Image.open(path0 + '/' + name)
            img = img.resize((HEIGHT,WIDTH))
            img = np.array(img)
            img = img/255
            images.append(img)

            name = (lines[i].split(';')[1]).replace("\n", "")
            img = Image.open(path1 + '/' + name)
            img = img.resize((int(HEIGHT),int(WIDTH)))
            img = np.array(img)
            
            seg_labels = label_map(img)
            seg_labels = np.reshape(seg_labels, (-1,NCLASSES))

            labels.append(seg_labels)
            i = (i+1) % n
        yield (np.array(images),np.array(labels))

# 定义损失函数
def loss(y_true, y_pred):
    crossloss = K.binary_crossentropy(y_true,y_pred)
    loss = K.sum(crossloss)/HEIGHT/WIDTH

    return loss

if __name__ == "__main__":
    
    # 用于最后保存模型的路径
    log_dir = ''

    # 创建模型
    model = Net()

    # 获取训练样本和验证样本的数目
    with open(path_train_txt, 'r') as f:
        lines = f.readlines()
    num_train = len(lines)
    with open(path_val_txt, 'r') as f:
        lines = f.readlines()
    num_val = len(lines)


    # 设置学习率下降方法,val_loss验证损失连续5个epoch不下降就让学习率减半
    reduce_lr = ReduceLROnPlateau(
                            monitor='val_loss', 
                            factor=0.5, 
                            patience=5, 
                            verbose=1
                        )
    # 是否需要早停，当val_loss一直不下降的时候意味着模型基本训练完毕，可以停止
    early_stopping = EarlyStopping(
                            monitor='val_loss', 
                            min_delta=0, 
                            patience=10, 
                            verbose=1
                        )
    # 设置损失，优化器
    model.compile(loss = loss,
            optimizer = Adam(lr=1e-3),
            metrics = ['accuracy'])

    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, BATCH_SIZE))

    # 开始训练
    model.fit_generator(data_generator('train'),
            steps_per_epoch=max(1, num_train//BATCH_SIZE),
            validation_data=data_generator('val'),
            validation_steps=max(1, num_val//BATCH_SIZE),
            epochs=50,
            initial_epoch=0,
            callbacks=[reduce_lr, early_stopping])

    model.save_weights(log_dir+'Dali.h5')


```

**[PyTorch实现](https://fuhao7i.com/2021/03/12/dalitools2/)**

```python
import argparse
import copy
import os
import time
import warnings

from PIL import Image
import torch
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import matplotlib.pyplot as plt

import torch.nn as nn

# ===================>
import logging
logging.basicConfig(filename='./work_dirs/segnet/segnet.log', filemode='a', level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logger.setLevel(level = logging.DEBUG)

formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

console = logging.StreamHandler()
console.setLevel(logging.INFO)
console.setFormatter(formatter)

logger.addHandler(console)
# <==================

# 标签像素值对应的物体类别, 0为背景
CLASSES = {
    '[0 0 0]' : 0,    # 背景
    '[22 0 255]' : 1, # 建筑物
    '[0 255 0]' : 2,  # 植被
    '[255 255 255]':3,# 道路
}

NCLASSES = 4
HEIGHT = 512
WIDTH = 512

BATCH_SIZE = 8

# train.txt和val.txt的文件路径
path_train_txt = './dataset/train.txt'
path_val_txt = './dataset/val.txt'

# train的图像和标签路径
path_Xtrain = './dataset/jpg/'
path_Xlabel = './dataset/png/'
# val的图像和标签路径
path_Yval = './dataset/val_jpg/'
path_Ylabel = './dataset/val_png/'



# labels映射
def label_map(labels):
    labelmap = np.zeros((int(HEIGHT),int(WIDTH),NCLASSES))
    for h in range(int(HEIGHT)):
        for w in range(int(WIDTH)):
            if str(labels[h, w]) in CLASSES.keys():
                c = CLASSES[str(labels[h, w])]
            else:
                c = 0
            labelmap[h, w, c] = 1
    return labelmap

class MyDataset(Dataset):  # 创建类：MyDataset,继承torch.utils.data.Dataset
    def __init__(self, datatxt, mode, transform=None):
        super(MyDataset, self).__init__()
        fh = open(datatxt, 'r')  # 打开txt，读取内容
        data = []
        for line in fh:  # 按行循环txt文本中的内容
            words = line.split(';')  # 通过指定分隔符对字符串进行切片
            data.append((words[0], words[1]))  # 把txt里的内容读入data列表保存，words[0]是图片信息，words[1]是label
        if mode == 'train':
            self.path_train = path_Xtrain
            self.path_label = path_Xlabel
        self.data = data
        self.transform = transform

    def __getitem__(self, index):  # 按照索引读取每个元素的具体内容
        fn, label = self.data[index]  # fn是图片path

        img = Image.open(self.path_train + fn).convert('RGB')

        label = Image.open(self.path_label + fn).convert('RGB')



        # from PIL import Image

        if self.transform is not None:  # 是否进行transform
            img = self.transform(img)
        return img, label  # return回哪些内容，在训练时循环读取每个batch，就能获得哪些内容

    def __len__(self):  # 它返回的是数据集的长度，必须有
        return len(self.data)


def main():


    
    # 实例化网络
    model = Net()
    # print(model)
    
    if torch.cuda.is_available():
        model =  model.cuda()

    train_transforms = transforms.Compose([
     
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5)),
        ])
    
    val_transforms = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5)),
    ])

    train_data = MyDataset(datatxt=path_train_txt, transform=train_transforms)
    train_loader = DataLoader(dataset=train_data, batch_size=2, shuffle=True)

    val_data = MyDataset(datatxt=path_val_txt, transform=val_transforms)
    val_loader = DataLoader(dataset=val_data, batch_size=2, shuffle=True)

    optimizer = optim.SGD(model.module.backbone.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)

    # print('backbone parameters:', model.module.backbone)
    num_epoches = 20
    image = {}
    criterion = nn.CrossEntropyLoss()

    val_acc_history = []
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    # pretrained_dict = torch.load('/content/drive/MyDrive/search/mmdetection/data/resneXt_imagenet.pth')
    # model_dict = model.state_dict()
    # pretrained_dict = {k: v for k, v in pretrained_dict.items() if 'classifier' not in k}
    # for k in pretrained_dict:
    #     print(k)
    # model_dict.update(pretrained_dict)
    # model.load_state_dict(model_dict)

    for epoch in range(num_epoches):
        print('Epoch {}/{}'.format(epoch + 1, num_epoches))
        print('-' * 10)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode

                running_loss = 0.0
                running_corrects = 0

                for i, (imgs, labels) in enumerate(train_loader):
                    imgs = imgs.cuda(cfg.gpu_ids[0])
                    labels = labels.cuda(cfg.gpu_ids[0])
                    
                    # print(labels)
                    optimizer.zero_grad()       

                    image['img'] = imgs
                    image['img_metas'] = imgs

                    outputs = model.extract_backbone(image, optimizer)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()
                    
                    _, preds = torch.max(outputs, 1)

                    print('loss:', loss.item())
                    running_loss += loss.item() * imgs.size(0)
                    running_corrects += torch.sum(preds == labels.data)
                    # print('{}/{}'.format(i * 50, len(train_loader.dataset)))

                epoch_loss = running_loss / len(train_loader.dataset)
                epoch_acc = running_corrects.double() / len(train_loader.dataset)

                print(' Loss: {:.4f}, acc: {:.6f} , running_loss: {:6f} , len: {:.6f}'.format(epoch_loss, epoch_acc, running_loss, len(train_loader.dataset)))    


            else:
                optimizer.zero_grad()     
                # if epoch < 10:
                #     break
                model.eval()   # Set model to evaluate mode
                
                running_loss = 0.0
                running_corrects = 0

                for i, (imgs, labels) in enumerate(val_loader):

                    imgs = imgs.cuda(cfg.gpu_ids[0])
                    labels = labels.cuda(cfg.gpu_ids[0])
                    # print(labels)
                    optimizer.zero_grad()

                    image['img'] = imgs
                    image['img_metas'] = imgs
                    
                    outputs = model.extract_backbone(image, optimizer)
                    # print(outputs.shape, labels.shape)
                    loss = criterion(outputs, labels)
                    
                    _, preds = torch.max(outputs, 1)


                    running_loss += loss.item() * imgs.size(0)
                    running_corrects += torch.sum(preds == labels.data)
                # epoch_loss = running_loss / 1000.0
                # epoch_acc = running_corrects.double() / 1000.0
                epoch_loss = running_loss / len(val_loader.dataset)
                epoch_acc = running_corrects.double() / len(val_loader.dataset)

                print(' Val_loss: {:.4f}, Val_acc: {:.6f} '.format(epoch_loss, epoch_acc))
            
                if epoch_acc > best_acc:
                    best_acc = epoch_acc
                    best_model_wts = copy.deepcopy(model.state_dict())
                    torch.save(best_model_wts, '/content/drive/MyDrive/search/mmdetection/data/middle.pth')
                val_acc_history.append(epoch_acc)

            print()

    print('Best val Acc: {:4f}'.format(best_acc))
              
    torch.save(best_model_wts, '/content/drive/MyDrive/search/mmdetection/data/resneXt_imagenet_338x600_best.pth')




if __name__ == '__main__':
    main()

```
