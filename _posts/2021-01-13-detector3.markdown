---
layout:     post
title:      "DetectorğŸ¯3â€”â€”mmdetection è®­ç»ƒæ–‡ä»¶è¯¦è§£train.py"
subtitle:   " \"mmdetection å¼€å§‹è®­ç»ƒæ¨¡å‹\""
date:       2021-01-13 18:54:00
author:     "fuhao7i"
header-img: "img/in-post/mubiaojiance.jpg"
catalog: true
tags:
    - DetectorğŸ¯
---

> mmdetection è¯¦è§£ï¼šè®­ç»ƒè‡ªå·±çš„æ¨¡å‹

# 1. train_detector() å‡½æ•°è¯¦è§£

```python
train_detector(
    model,
    train_dataset,
    cfg,
    distributed=distributed,
    validate=args.validate,
    logger=logger)
```

`å‚æ•°ï¼š`

```Bash
model : æ„å»ºçš„ç½‘ç»œæ¨¡å‹
train_dataset : æ„å»ºçš„è®­ç»ƒæ•°æ®é›†
cfg : è¯»å–çš„Config pyæ–‡ä»¶
distributed : æ˜¯å¦æ˜¯åˆ†å¸ƒå¼è®­ç»ƒ
validate : whether to evaluate the checkpoint during training
logger : æ—¥å¿—ä¿¡æ¯
```

æ¥ä¸‹æ¥æˆ‘ä»¬è¯¦ç»†çœ‹ä¸€ä¸‹`train_detector()å‡½æ•°`ã€‚  
`./mmdet/apis/train.py`

```python
def train_detector(model,
                   dataset,
                   cfg,
                   distributed=False,
                   validate=False,
                   logger=None):
    if logger is None:
        logger = get_root_logger(cfg.log_level)
    
    # start training
    if distributed:
        _dist_train(model, dataset, cfg, validate=validate)
    else:
        _non_dist_train(model, dataset, cfg, validate=validate)
```

## 1.1 _non_dist_train() (å•ä¸ªGPU)éåˆ†å¸ƒå¼è®­ç»ƒ

```python
def _non_dist_train(model, dataset, cfg, validate=False):
    if validate:
        raise NotImplementedError('Built-in validation is not implemented '
                                  'yet in not-distributed training. Use '
                                  'distributed training or test.py and '
                                  '*eval.py scripts instead.')
    # put model on gpus
    model = MMDataParallel(model, device_ids=range(cfg.gpus)).cuda()
    
    # build runner
    optimizer = build_optimizer(model, cfg.optimizer, cfg.get('optimizer_exclude_arch'))

    arch_name = None
    optimizer_arch = None
    if 'optimizer_arch' in cfg:
        raise NotImplementedError
    
    runner = Runner(model, batch_processor, optimizer, optimizer_arch, cfg.work_dir, cfg.log_level, arch_name=arch_name)

    # fp16 setting
    fp16_cfg = cfg.get('fp16', None)
    if fp16_cfg is not None:
        optimizer_config = Fp16OptimizerHook(
            **cfg.optimizer_config, **fp16_cfg, distributed=False)
    else:
        optimizer_config = cfg.optimizer_config
        optimizer_arch_config = cfg.optimizer_config
    runner.register_training_hooks(cfg.lr_config, optimizer_config, optimizer_arch_config,
                                   cfg.checkpoint_config, cfg.log_config)

    if cfg.resume_from:
        runner.resume(cfg.resume_from)
    elif cfg.load_from:
        runner.load_checkpoint(cfg.load_from)
    
    if 'optimizer_arch' in cfg:
        raise NotImplementedError
    else:
        data_loaders = [
            build_dataloader(
                dataset,
                cfg.data.imgs_per_gpu,
                cfg.data.workers_per_gpu,
                cfg.gpus,
                dist=False)
        ]
        runner.run(data_loaders, None, cfg.workflow, cfg.total_epochs)
```

### 1.1.1 æ„å»ºRunnerå®ä¾‹

```python
runner = Runner(model, batch_processor, optimizer, optimizer_arch, cfg.work_dir, cfg.log_level, arch_name=arch_name)
...
runner.run(data_loaders, None, cfg.workflow, cfg.total_epochs)
```

è¿™é‡Œæˆ‘ä»¬æ¥çœ‹`Runner`ç¬¬äºŒä¸ªå‚æ•°`batch_processor`

```python
def batch_processor(model, data, train_mode, **kwargs):
    losses = model(**data)

    losses_ = losses[0]
    loss_latency = losses[1]
    if loss_latency is not None:
        losses_['loss_latency'] = loss_latency

    loss, log_vars = parse_losses(losses_)
   
    outputs = dict(
        loss=loss, log_vars=log_vars, num_samples=len(data['img'].data))

    return outputs
```
