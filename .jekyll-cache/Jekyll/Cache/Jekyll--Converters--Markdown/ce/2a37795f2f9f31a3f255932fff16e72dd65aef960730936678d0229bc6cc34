I"<blockquote>
  <p>现代神经网络是一种非线性统计性数据建模工具。</p>
</blockquote>

<h1 id="1-为什么需要激活函数">1. 为什么需要激活函数？</h1>

<ol>
  <li>激活函数可以给模型引入非线性的因素。</li>
  <li>假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。</li>
  <li>使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物。使用非线性激活函数，能够从输入输出之间生成非线性映射。</li>
</ol>

<h1 id="2-损失函数的确定与损失计算">2. 损失函数的确定与损失计算</h1>

<blockquote>
  <p>确定一个能表述label和预测值之间误差值的损失函数，然后尽可能的通过更新权重w来优化这个损失函数，以达到提高预测准确率的目的。</p>
</blockquote>

<h1 id="3-梯度">3. 梯度</h1>

<p>所谓梯度其实就是一个偏导数向量，但是我们经常说的仍是<code class="language-plaintext highlighter-rouge">x的梯度</code>而不是<code class="language-plaintext highlighter-rouge">x的偏导数（就是对x求偏导）</code>。</p>

<h1 id="4-反向传播">4. 反向传播</h1>

<blockquote>
  <p>作用：对损失函数优化，将损失值降到最低.</p>
</blockquote>

<p><code class="language-plaintext highlighter-rouge">反向传播算法就是梯度下降的求导链式法则的应用。</code></p>

<p>简单的说，就是损失函数E对每一个权重参数wi求导，然后用这个求得的导数去更新wi自身，因为在神经网络的隐藏层和输出层中存在不止一个层次的权重wi，所以需要从输出层开始向前一层一层的求导，这就是为什么叫做反向传播的原因。</p>

<p>以w10为例，要更新w10，需要先求得E对w10的导数。</p>

<p><img src="https://img-blog.csdnimg.cn/2021031114493974.gif" />
<img src="https://img-blog.csdnimg.cn/2021031114493976.gif" /></p>

<p>反向传播中，每一次迭代进行一次全体权重参数更新，直到参数不再更新，即得到全局最小值或局部最小值。</p>
:ET