<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>fuhao7i Blog</title>
    <description>这里是 @fuhao7i 的个人博客，我还很年轻，吃苦趁现在！</description>
    <link>https://fuhao7i.github.io/</link>
    <atom:link href="https://fuhao7i.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 13 Apr 2021 21:20:59 +0800</pubDate>
    <lastBuildDate>Tue, 13 Apr 2021 21:20:59 +0800</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Dali杂货铺🐰20——Common operations of tensors</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;record the common operations of tensors.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;1-torchcattensor1-tensor2-dim&quot;&gt;1. torch.cat((tensor1, tensor2), dim)&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;cat is ‘concatenate’, menas link (things) together in a chain or series.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&quot;language-Bash&quot;&gt;&amp;gt;&amp;gt;&amp;gt; import torch
&amp;gt;&amp;gt;&amp;gt; A=torch.ones(2,3) #2x3的张量（矩阵）                                     
&amp;gt;&amp;gt;&amp;gt; A
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])
&amp;gt;&amp;gt;&amp;gt; B=2*torch.ones(4,3)#4x3的张量（矩阵）                                    
&amp;gt;&amp;gt;&amp;gt; B
tensor([[ 2.,  2.,  2.],
        [ 2.,  2.,  2.],
        [ 2.,  2.,  2.],
        [ 2.,  2.,  2.]])
&amp;gt;&amp;gt;&amp;gt; C=torch.cat((A,B),0)#按维数0（行）拼接
&amp;gt;&amp;gt;&amp;gt; C
tensor([[ 1.,  1.,  1.],
         [ 1.,  1.,  1.],
         [ 2.,  2.,  2.],
         [ 2.,  2.,  2.],
         [ 2.,  2.,  2.],
         [ 2.,  2.,  2.]])
&amp;gt;&amp;gt;&amp;gt; C.size()
torch.Size([6, 3])
&amp;gt;&amp;gt;&amp;gt; D=2*torch.ones(2,4) #2x4的张量（矩阵）
&amp;gt;&amp;gt;&amp;gt; C=torch.cat((A,D),1)#按维数1（列）拼接
&amp;gt;&amp;gt;&amp;gt; C
tensor([[ 1.,  1.,  1.,  2.,  2.,  2.,  2.],
        [ 1.,  1.,  1.,  2.,  2.,  2.,  2.]])
&amp;gt;&amp;gt;&amp;gt; C.size()
torch.Size([2, 7])
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Wed, 14 Apr 2021 05:07:00 +0800</pubDate>
        <link>https://fuhao7i.github.io/2021/04/14/sundry20/</link>
        <guid isPermaLink="true">https://fuhao7i.github.io/2021/04/14/sundry20/</guid>
        
        <category>Dali杂货铺🐰</category>
        
        
      </item>
    
      <item>
        <title>Detector🎯5—— one-stage VS two-stage</title>
        <description>&lt;h1 id=&quot;1-main-differences&quot;&gt;1. main differences&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;one-stage network is faster.&lt;/li&gt;
  &lt;li&gt;two-stage network is more accurate.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2-one-stage&quot;&gt;2. one-stage&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Directly regress the category probability and position coordinate value of the object without RPN(region proposal network).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;classical one-stage object detection network&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Bash&quot;&gt;YOLOv1, YOLOv2, YOLOv3
SSD, DSSD ...
Retina-Net ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;retina-net&quot;&gt;&lt;a href=&quot;https://blog.csdn.net/cp1314971/article/details/105841094/&quot;&gt;Retina Net&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20210413164117265.png&quot; center=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/kuangliu/pytorch-retinanet&quot;&gt;PyTorch-RetinaNet&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 14 Apr 2021 02:50:00 +0800</pubDate>
        <link>https://fuhao7i.github.io/2021/04/14/detector5/</link>
        <guid isPermaLink="true">https://fuhao7i.github.io/2021/04/14/detector5/</guid>
        
        <category>Detector🎯</category>
        
        
      </item>
    
      <item>
        <title>Neural Network🦖8——model.eval() VS with torch.no_grad()</title>
        <description>&lt;p&gt;&lt;strong&gt;In short&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;model.eval()&lt;/strong&gt; will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;torch.no_grad()&lt;/strong&gt; impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop(which you don’t want in an eval script).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;reference&quot;&gt;reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615&quot;&gt;‘model.eval()’ vs ‘with torch.no_grad()’&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 13 Apr 2021 18:12:00 +0800</pubDate>
        <link>https://fuhao7i.github.io/2021/04/13/nn8/</link>
        <guid isPermaLink="true">https://fuhao7i.github.io/2021/04/13/nn8/</guid>
        
        <category>Neural Network🦖</category>
        
        
      </item>
    
      <item>
        <title>Dali杂货铺🐰19——Rules for defining the names of hyperparameters in torch</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Sometimes, we need to train models jointly, or use one model to help another model training. So it’s critically for us to konw the name of parameters in the model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;monet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ModuleList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arch_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arch_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;monet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arch_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;outputs:&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Bash&quot;&gt;pre_conv.0.weight
pre_conv.0.bias
conv0.weight
conv0.bias
layers.0.weight
layers.0.bias
layers.1.weight
layers.1.bias
layers.2.weight
layers.2.bias
[tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)]
Sequential(
  (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace=True)
)
&amp;lt;generator object Module.parameters at 0x7f22dfa5bb50&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Sat, 10 Apr 2021 23:46:00 +0800</pubDate>
        <link>https://fuhao7i.github.io/2021/04/10/sundry19/</link>
        <guid isPermaLink="true">https://fuhao7i.github.io/2021/04/10/sundry19/</guid>
        
        <category>Dali杂货铺🐰</category>
        
        
      </item>
    
      <item>
        <title>Neural Network🦖7——How to clip gradient?</title>
        <description>&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Due to the improper selection of the learning rate, the weight update is larger.&lt;/li&gt;
    &lt;li&gt;There is a lot of noise in the prepared data, resulting in large differences in target variables.&lt;/li&gt;
    &lt;li&gt;The improper selection of the loss function results in the calculation of a larger error value.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;梯度爆炸/消失的一种常见解决方法是重新缩放误差导数，通过网络反向传播误差导数，然后使用它来更新权重。通过重新缩放误差导数，权重的更新也将被重新缩放，从而大大降低了上溢(NaN)或下溢(Inf)的可能性。更新误差导数的主要方法有两种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;梯度缩放 Gradient Scaling&lt;/li&gt;
  &lt;li&gt;梯度裁剪 Gradient Clipping&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;梯度缩放涉及对误差梯度向量进行归一化，以使向量范数大小等于定义的值，例如1.0。只要它们超过阈值，就重新缩放它们。如果渐变超出了预期范围，则渐变裁剪会强制将渐变值（逐个元素）强制为特定的最小值或最大值。这些方法通常简称为梯度裁剪。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;它是一种仅解决训练深度神经网络模型的数值稳定性，而不能改进网络性能的方法。&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;implementation-by-torch&quot;&gt;implementation by torch&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.utils.clip_grad_norm(parameters, max_norm=8, norm_type=2)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这个函数是根据参数的范数来衡量的&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;parameters: 一个基于变量的迭代器，会进行归一化. an iterable of Variables that will have gradients normalized&lt;/li&gt;
  &lt;li&gt;max_norm: 梯度最大范数 max norm of the gradients&lt;/li&gt;
  &lt;li&gt;norm_type: 规定范数的类型，默认为L2. type of the used p-norm. Can be’inf’for infinity norm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;returns: 参数的总体范数(作为单个向量来看)&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;reference&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_39653948/article/details/105962326&quot;&gt;【调参19】如何使用梯度裁剪（Gradient Clipping）避免梯度爆炸&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 07 Apr 2021 19:12:00 +0800</pubDate>
        <link>https://fuhao7i.github.io/2021/04/07/nn7/</link>
        <guid isPermaLink="true">https://fuhao7i.github.io/2021/04/07/nn7/</guid>
        
        <category>Neural Network🦖</category>
        
        
      </item>
    
      <item>
        <title>Dali杂货铺🐰18——The role of hook in mmdetection</title>
        <description>&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/238130913&quot;&gt;目标检测(MMdetection)-HOOK机制&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 07 Apr 2021 18:02:00 +0800</pubDate>
        <link>https://fuhao7i.github.io/2021/04/07/sundry18/</link>
        <guid isPermaLink="true">https://fuhao7i.github.io/2021/04/07/sundry18/</guid>
        
        <category>Dali杂货铺🐰</category>
        
        
      </item>
    
      <item>
        <title>Dali杂货铺🐰17——torch 训练过程查看某一层的梯度</title>
        <description>&lt;p&gt;在&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss.backward()&lt;/code&gt;之后查看某一层的梯度&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'=== after loss.backward() ==='&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Mon, 05 Apr 2021 23:58:00 +0800</pubDate>
        <link>https://fuhao7i.github.io/2021/04/05/sundry17/</link>
        <guid isPermaLink="true">https://fuhao7i.github.io/2021/04/05/sundry17/</guid>
        
        <category>Dali杂货铺🐰</category>
        
        
      </item>
    
      <item>
        <title>Dali杂货铺🐰16——Summary of drawing neural network tools</title>
        <description>&lt;h1 id=&quot;1-plotneuralnet&quot;&gt;1. &lt;a href=&quot;https://github.com/HarisIqbal88/PlotNeuralNet&quot;&gt;plotNeuralNet&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20210405111744890.png&quot; center=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-bokeh&quot;&gt;2. &lt;a href=&quot;https://bokeh.pydata.org/en/latest/&quot;&gt;Bokeh&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://bokeh.pydata.org/en/latest/docs/user_guide.html#userguide&quot;&gt;Bokeh官方文档&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Bokeh是一个交互式可视化库，面向现代Web浏览器进行演示。 它的目标是提供优雅，简洁的多功能图形构造，并通过非常大或流数据集的高性能交互来扩展此功能。 Bokeh可以帮助任何想要快速轻松地创建交互式图表，仪表板和数据应用程序的人。&lt;/p&gt;
</description>
        <pubDate>Mon, 05 Apr 2021 19:16:00 +0800</pubDate>
        <link>https://fuhao7i.github.io/2021/04/05/sundry16/</link>
        <guid isPermaLink="true">https://fuhao7i.github.io/2021/04/05/sundry16/</guid>
        
        <category>Dali杂货铺🐰</category>
        
        
      </item>
    
      <item>
        <title>Neural Network🦖6——How dose the convolutional layer achieve higher or lower dimensionality?</title>
        <description>&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;The convolution kernel not only has height and width but also has depth&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2021040509574565.png&quot; center=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And it has the same depth as the feature map being convolved. Therefore, each convolution kernel can traverse all the feature maps of the upper level.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20210405100253974.jpeg&quot; center=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每一个卷积核遍历完所有的特征图之后，进行线性相加，就得到了新的特征图。
After each convolution kernel traverses all the feature maps, linear addition is performed to obtain a new feature map.&lt;/p&gt;
</description>
        <pubDate>Mon, 05 Apr 2021 17:53:00 +0800</pubDate>
        <link>https://fuhao7i.github.io/2021/04/05/nn6/</link>
        <guid isPermaLink="true">https://fuhao7i.github.io/2021/04/05/nn6/</guid>
        
        <category>Neural Network🦖</category>
        
        
      </item>
    
      <item>
        <title>Dali杂货铺🐰15——loss不能正常下降 or 下降到一定程度便不下降了</title>
        <description>&lt;h1 id=&quot;1-分析如何解决神经网络训练时loss不下降的问题&quot;&gt;1. &lt;a href=&quot;https://blog.ailemon.me/2019/02/26/solution-to-loss-doesnt-drop-in-nn-train/&quot;&gt;分析:如何解决神经网络训练时loss不下降的问题&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;1-学习率没有灵魂&quot;&gt;1. 学习率没有灵魂&lt;/h2&gt;

&lt;p&gt;学习率是个神奇的东西, 你是不是也对learning rate的选取而苦恼过, 总是感觉一成不变的学习率缺少灵魂:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;太小？ loss降低的太慢💦&lt;/li&gt;
  &lt;li&gt;太大？ loss可能达不到最优, 而可能在最优值范围震动&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20210403160419334.png&quot; center=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;解决-torchoptimlr_scheduler-学习率下降机制&quot;&gt;解决: torch.optim.lr_scheduler 学习率下降机制&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&quot;&gt;How to adjust learning rate&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-学习率为0&quot;&gt;2. 学习率为0&lt;/h2&gt;

&lt;p&gt;学习率为0，网络的参数不再更新，故损失也不会再下降，因此我们可以看一下是不是错用了学习率下降方法导致学习率为0.&lt;/p&gt;

&lt;h2 id=&quot;3-优化器设置&quot;&gt;3. 优化器设置&lt;/h2&gt;

&lt;p&gt;看看我们的模型参数是不是正常载入到优化器中了。也就是看一下优化器的设置有没有出错。&lt;/p&gt;

&lt;h1 id=&quot;引用&quot;&gt;引用.&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/fufu_good/article/details/104340036&quot;&gt;使用Pytorch实现学习率衰减/降低（learning rate decay）&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/qyhaill/article/details/103043637&quot;&gt;torch.optim.lr_scheduler：调整学习率&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 03 Apr 2021 04:50:00 +0800</pubDate>
        <link>https://fuhao7i.github.io/2021/04/03/sundry15/</link>
        <guid isPermaLink="true">https://fuhao7i.github.io/2021/04/03/sundry15/</guid>
        
        <category>Dali杂货铺🐰</category>
        
        
      </item>
    
  </channel>
</rss>
